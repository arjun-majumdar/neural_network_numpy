{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Neural_Network_with_zero_weights.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNobA_gUhIuV",
        "colab_type": "text"
      },
      "source": [
        "An example code of a neural network having a single hidden layer for a binary classification using all weights as zeros!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqVoZ0P1hQx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJWm3FcZhTnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e8b07c6-4538-4077-8f7e-988df4966f4b"
      },
      "source": [
        "# Create blobs as synthetic dataset-\n",
        "X, y = make_blobs(n_samples=100000, centers=2, n_features=3)\n",
        "\n",
        "# Get shape of features (X) and label (y)-\n",
        "X.shape, y.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100000, 3), (100000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBcAz_sYhVYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "343d6842-b7d4-413a-dd52-ae83207f03f0"
      },
      "source": [
        "# Get distribution of label 'y'-\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "\n",
        "# Create a dictionary such that-\n",
        "# element: count\n",
        "element_count = dict(zip(unique, counts))\n",
        "\n",
        "element_count"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 50000, 1: 50000}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n52W-knXhYK_",
        "colab_type": "text"
      },
      "source": [
        "Since this is a synthetic dataset, the distribution of label 'y' is uniform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFpX02Jkhf6D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eaa06330-b0f1-4da4-e202-8571ef47a855"
      },
      "source": [
        "# Split features (X) and label (y) into training and\n",
        "# testing sets-\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((70000, 3), (70000,), (30000, 3), (30000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjfRdNXohhr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "\t'''\n",
        "\tSigmoid activation function\n",
        "\t'''\n",
        "\ts = 1 / (1 + np.exp(-z))\n",
        "\treturn s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-dPABK3hj60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):\n",
        "\t'''\n",
        "\tFunction to calculate ReLU for\n",
        "\tgiven 'x'\n",
        "\t'''\n",
        "\treturn np.maximum(x, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcpyllRWhl1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_derivative(x):\n",
        "\t'''\n",
        "\tFunction to calculate derivative\n",
        "\tof ReLU\n",
        "\t'''\n",
        "\treturn np.where(x <= 0, 0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUBhtSRUhnfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh_derivative(x):\n",
        "\t'''\n",
        "\tFunction to calculate derivative of hyperbolic tangent\n",
        "\tfor given parameter 'x'\n",
        "\tUsed as activation function for hidden layer\n",
        "\t'''\n",
        "\treturn (1 - np.power(np.tanh(x), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "107U7ag1ho_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(input_layer, hidden_layer, output_layer):\n",
        "\t'''\n",
        "\tFunction to initialize parameters for a neural network with-\n",
        "\t'input_layer' number of neurons in input layer\n",
        "\t'hidden_layer' number of neurons in hidden layer\n",
        "\t'output_layer' neuron in output layer [output layer has one neuron]\n",
        "\n",
        "\tInitialize weights as small numbers!\n",
        "\t'''\n",
        "\t# Initialize weights for hidden layer and input layer-\n",
        "\tW1 = np.random.randn(hidden_layer, input_layer) * 0.01\n",
        "\n",
        "\t# Initialize bias values for hidden layer-\n",
        "\t# b1 = np.zeros((hidden_layer, 1))\n",
        "\t# OR-\n",
        "\t# b1 = np.random.rand(hidden_layer, 1) * 0.01\n",
        "\tb1 = np.random.randn(1, hidden_layer) * 0.01\n",
        "\n",
        "\t# Initialize weights for output layer and hidden layer-\n",
        "\tW2 = np.random.randn(output_layer, hidden_layer) * 0.01\n",
        "\n",
        "\t# Initialize bias values for output layer-\n",
        "\t# b2 = np.zeros((output_layer, 1))\n",
        "\t# OR-\n",
        "\t# b2 = np.random.rand(output_layer, 1) * 0.01\n",
        "\tb2 = np.random.randn(1, output_layer) * 0.01\n",
        "\n",
        "\t# Return all weights and biases as a dictionary object-\n",
        "\tparameters = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "\n",
        "\treturn parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXr_IzMvhsPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_zeros(input_layer, hidden_layer, output_layer):\n",
        "\t'''\n",
        "\tFunction to initialize parameters for a neural network with-\n",
        "\t'input_layer' number of neurons in input layer\n",
        "\t'hidden_layer' number of neurons in hidden layer\n",
        "\t'output_layer' neuron in output layer [output layer has one neuron]\n",
        "\n",
        "\tInitialize ALL weights & biases to zero!\n",
        "\t'''\n",
        "\n",
        "\t# Initialize weights for hidden layer and input layer-\n",
        "\t# W1 = np.random.randn(hidden_layer, input_layer) * 0.01\n",
        "\tW1 = np.zeros(shape = (hidden_layer, input_layer))\n",
        "\n",
        "\t# Initialize bias values for hidden layer-\n",
        "\t# b1 = np.zeros((hidden_layer, 1))\n",
        "\t# OR-\n",
        "\t# b1 = np.random.rand(hidden_layer, 1) * 0.01\n",
        "\tb1 = np.random.randn(1, hidden_layer) * 0.01\n",
        "\t# b1 = np.zeros(shape = (1, hidden_layer))\n",
        "\n",
        "\t# Initialize weights for output layer and hidden layer-\n",
        "\t# W2 = np.random.randn(output_layer, hidden_layer) * 0.01\n",
        "\tW2 = np.zeros(shape = (output_layer, hidden_layer))\n",
        "\n",
        "\t# Initialize bias values for output layer-\n",
        "\t# b2 = np.zeros((output_layer, 1))\n",
        "\t# OR-\n",
        "\t# b2 = np.random.rand(output_layer, 1) * 0.01\n",
        "\tb2 = np.random.randn(1, output_layer) * 0.01\n",
        "\t# b2 = np.zeros(shape = (1, output_layer))\n",
        "\n",
        "\t# Return all weights and biases as a dictionary object-\n",
        "\tparameters = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "\n",
        "\treturn parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYHu1pTdhuur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_and_backward_propagation(X, parameters, Y):\n",
        "\t'''\n",
        "\tFunction to compute forward propagation based on 'X', 'Y'\n",
        "\tand 'parameters' dictionary\n",
        "\n",
        "\tInput:\n",
        "\t1.) 'X' a numpy array \n",
        "\t2.) 'Y' a numpy array\n",
        "\t3.) 'parameters' a Python dictionary containing weights & biases\n",
        "\n",
        "\n",
        "\tReturns:\n",
        "\t1.) gradients for weights and biases\n",
        "\t2.) cost\n",
        "\n",
        "\tZ1 is network input for hidden layer\n",
        "\tA1 is output of hidden layer (tanh output)\n",
        "\tZ2 is network input for output layer\n",
        "\tA2 is output of output layer (sigmoid output)\n",
        "\t'''\n",
        "\n",
        "\t# Retreive initialized weights & biases-\n",
        "\tW1 = parameters['W1']\n",
        "\tb1 = parameters['b1']\n",
        "\tW2 = parameters['W2']\n",
        "\tb2 = parameters['b2']\n",
        "\n",
        "\n",
        "\t# Implement forward propagation to compute A2 probabilities-\n",
        "\n",
        "\t# Network input for hidden layer-\n",
        "\tZ1 = np.dot(W1, X.T) + b1.T\t\t# (hidden_layer, m)\n",
        "\n",
        "\t# Using tanh as activation function for hidden layer-\n",
        "\tA1 = np.tanh(Z1)\t# (output_layer, m)\n",
        "\n",
        "\t# Using ReLU as activation function for hidden layer-\n",
        "\t# A1 = np.maximum(Z1, 0)\n",
        "\n",
        "\t# Network input for output layer-\n",
        "\tZ2 = np.dot(W2, A1) + b2\t# (n_o, m) OR (1, m)\n",
        "\n",
        "\t# Using sigmoid activation function for output layer\n",
        "\tA2 = sigmoid(Z2)\t# (1, m) OR (n_o, m)\n",
        "\n",
        "\t# Number of training examples-\n",
        "\tm = X.shape[0]\n",
        "\n",
        "\t# Implement backward propagation for adjusting weights and biases-\n",
        "\n",
        "\t# Partial derivative of cost wrt W2-\n",
        "\tdJ_dW2 = (1 / m) * np.dot((A2 - Y), A1.T)\n",
        "\n",
        "\t\"\"\"\n",
        "\t# Sanity check-\n",
        "\tif dJ_dW2.shape == W2.shape:\n",
        "\t\tprint(\"\\nW2.shape equals dJ_dW2.shape\\n\")\n",
        "\telse:\n",
        "\t\tprint(\"\\nW2.shape is NOT equal to dJ_dW2.shape! Recheck!\\n\")\n",
        "\t\"\"\"\n",
        "\n",
        "\t# Partial derivative of cost wrt W1 using tanh activation function-\n",
        "\t# dJ_dW1 = (1 / m) * np.dot(np.multiply(np.dot(W2.T, (A2 - Y)), (1 - np.square(A1))), X)\n",
        "\n",
        "\t# Partial derivative of cost wrt W1 using ReLU activation function-\n",
        "\tdJ_dW1 = (1 / m) * np.dot(np.multiply(np.dot(W2.T, (A2 - Y)), relu_derivative(A1)), X)\n",
        "\n",
        "\t# Sanity check-\n",
        "\t# dJ_dW1.shape == W1.shape\n",
        "\t# True\n",
        "\n",
        "\t# Partial detivative of cost wrt hidden layer bias (b1)- using tanh activation function-\n",
        "\t# dJ_db1 = (1 / m) * np.multiply(np.dot((A2 - Y), (1 - np.square(A1)).T), W2)\n",
        "\n",
        "\t# Partial derivative of cost wrt hidden layer bias (b1) using ReLU activation function-\n",
        "\tdJ_db1 = (1 / m) * np.multiply(np.dot((A2 - Y), relu_derivative(A1).T), W2)\n",
        "\n",
        "\t# Sanity check-\n",
        "\t# dJ_db1.shape == b1.shape\n",
        "\t# True\n",
        "\n",
        "\t# Partial derivative of cost wrt output layer bias (b2)-\n",
        "\tdJ_db2 = (1 / m) * np.sum(A2 - Y)\n",
        "\t# Returns a scalar with shape ()\n",
        "\n",
        "\tcost = compute_cost(A2, Y)\n",
        "\n",
        "\tgradient = {\n",
        "\t\t'dJ_dW1': dJ_dW1, 'dJ_dW2': dJ_dW2,\n",
        "\t\t'dJ_db1': dJ_db1, 'dJ_db2': dJ_db2\n",
        "\t}\n",
        "\n",
        "\treturn gradient, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4RPuARThySk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(A2, Y):\n",
        "\t'''\n",
        "\tFunction to compute cost using binary cross-entropy\n",
        "\tloss\n",
        "\n",
        "\tArguments-\n",
        "\tA2- Sigmoid output of output layer; shape- (1, m)\n",
        "\tY-  groud truth labels vector; shape- (1, m)\n",
        "\t# parameters- Python dict containing W1, b1, W2, b2\n",
        "\tm - number of training examples\n",
        "\n",
        "\tReturn-\n",
        "\tBinary cross-entropy cost\n",
        "\t'''\n",
        "\n",
        "\t# Number of training examples-\n",
        "\t# m = Y.shape[1]\n",
        "\tm = Y.shape[0]\n",
        "\n",
        "\t# Compute binary cross-entropy cost-\n",
        "\tlogprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
        "\t\n",
        "\tcost = (-1 / m) * np.sum(logprobs)\n",
        "\n",
        "\t# np.multiply() multiplies arguments element-wise\n",
        "\t# np.log() natural logarithm, element-wise\n",
        "\n",
        "\t# makes sure cost is the dimension we expect, E.g., turns [[51]] into 51-\n",
        "\tcost = np.squeeze(cost)\n",
        "\n",
        "\treturn cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H4VB6jTh0n8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimization(wts_bias_parameters, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "\t'''\n",
        "\tFunction to perform optimization to learn weight 'W' and bias 'b'\n",
        "\tby using gradient descent algorithm\n",
        "\n",
        "\tReturns the learnt 'W' and 'b' parameters AFTER training on training data\n",
        "\t'''\n",
        "\n",
        "\t# List variable to hold cost/loss-\n",
        "\tcosts = []\n",
        "\n",
        "\tfor i in range(num_iterations):\n",
        "\t\t# Compute gradients and cost using defined function-\n",
        "\t\tgradients, cost = forward_and_backward_propagation(X, wts_bias_parameters, Y)\n",
        "\n",
        "\t\t# Get partial derivates from 'gradients'\n",
        "\t\tdJ_db1 = gradients['dJ_db1']\n",
        "\t\tdJ_db2 = gradients['dJ_db2']\n",
        "\t\tdJ_dW2 = gradients['dJ_dW2']\n",
        "\t\tdJ_dW1 = gradients['dJ_dW1']\n",
        "\n",
        "\t\tW1 = wts_bias_parameters['W1']\n",
        "\t\tW2 = wts_bias_parameters['W2']\n",
        "\t\tb1 = wts_bias_parameters['b1']\n",
        "\t\tb2 = wts_bias_parameters['b2']\n",
        "\n",
        "\t\t# Update weights-\n",
        "\t\tW1 = W1 - (learning_rate * dJ_dW1)\n",
        "\t\tW2 = W2 - (learning_rate * dJ_dW2)\n",
        "\n",
        "\t\t# Update biases-\n",
        "\t\tb1 = b1 - (learning_rate * dJ_db1)\n",
        "\t\tb2 = b2 - (learning_rate * dJ_db2)\n",
        "\n",
        "\t\t# Update 'wts_bias_parameters' dict for next call-\n",
        "\t\twts_bias_parameters = {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
        "\n",
        "\t\t# Store loss/cost AFTER every 100 iterations-\n",
        "\t\tif i % 100 == 0:\n",
        "\t\t\tcosts.append(cost)\n",
        "\n",
        "\t\t# Print cost AFTER every 100 iterations-\n",
        "\t\tif print_cost and i % 100 == 0:\n",
        "\t\t\tprint(\"\\nLoss/Cost after {0} iterations = {1:.4f}\\n\".format(i, cost))\n",
        "\n",
        "\n",
        "\t# Update computed weights and biases-\n",
        "\t# The actual weights and biases AFTER training is done\n",
        "\tupdated_params = {'W1': W1, 'W2': W2, \n",
        "\t\t\t\t'b1': b1, 'b2': b2}\n",
        "\n",
        "\t# Update partial derivatives as a dictionary\n",
        "\t# Partial derivatives AFTER training is done-\n",
        "\tupdated_gradients = {'dJ_dW2': dJ_dW2, 'dJ_dW1': dJ_dW1,\n",
        "\t\t\t'dJ_db1': dJ_db1, 'dJ_db2': dJ_db2}\n",
        "\n",
        "\n",
        "\t# Return everything-\n",
        "\treturn updated_params, updated_gradients, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHT7K4rqh3Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(wts_bias_parameters, X):\n",
        "\t'''\n",
        "\tFunction to make predictions using trained weights and biases\n",
        "\n",
        "\t\n",
        "\t'''\n",
        "\n",
        "\t# Number of examples-\n",
        "\tm = X.shape[0]\n",
        "\n",
        "\ty_pred = np.zeros((1, m))\t# shape- (1, m)\n",
        "\n",
        "\tW1 = wts_bias_parameters['W1']\n",
        "\tW2 = wts_bias_parameters['W2']\n",
        "\tb1 = wts_bias_parameters['b1']\n",
        "\tb2 = wts_bias_parameters['b2']\n",
        "\n",
        "\t# Perform forward propagation to predict for given 'X'-\n",
        "\tZ1 = np.dot(W1, X.T) + b1.T \t# (output_layer, m)\n",
        "\tA1 = np.tanh(Z1)\t# (output_layer, m)\n",
        "\tZ2 = np.dot(W2, A1) + b2\t# (n_o, m) OR (1, m)\n",
        "\tA2 = sigmoid(Z2)\n",
        "\n",
        "\tfor i in range(A2.shape[1]):\n",
        "\t\t# Convert probabilities A2[0, i] to binary labels-\n",
        "\t\tif A2[0, i] > 0.5:\n",
        "\t\t\ty_pred[0, i] = 1\t\t# 1 is for CAT\n",
        "\t\telse:\n",
        "\t\t\ty_pred[0, i] = 0\t\t# 0 is for DOG\n",
        "\n",
        "\treturn y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqrI4drfh5n0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7459fdf0-dd6f-4c48-cf4c-6ca3fcb9c0dc"
      },
      "source": [
        "print(\"\\nDimensions of training set are:\")\n",
        "print(\"X_train.shape = {0} & y_train.shape = {1}\\n\".format(X_train.shape, y_train.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dimensions of training set are:\n",
            "X_train.shape = (70000, 3) & y_train.shape = (70000,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4dYc7CZh7qk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a7bc9734-77e1-4d3b-be43-7781c37df8c4"
      },
      "source": [
        "print(\"\\nDimensions of testing set are:\")\n",
        "print(\"X_test.shape = {0} & y_test.shape = {1}\\n\".format(X_test.shape, y_test.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dimensions of testing set are:\n",
            "X_test.shape = (30000, 3) & y_test.shape = (30000,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs37SXs5h9oB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize parameters by specifying number of neurons in each layers-\n",
        "n_hidden_neurons = 10\n",
        "\n",
        "initialized_params = initialize_parameters(X_train.shape[1], n_hidden_neurons, 1)\n",
        "# 'initialized_params' is a dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkk9fG3WiCDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "27f1eb03-ad51-487d-b9a1-4a1d6db86e1a"
      },
      "source": [
        "for layer in initialized_params.keys():\n",
        "\tprint(\"{0} layer has shape = {1}\".format(layer, initialized_params[layer].shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 layer has shape = (10, 3)\n",
            "b1 layer has shape = (1, 10)\n",
            "W2 layer has shape = (1, 10)\n",
            "b2 layer has shape = (1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_5XMEa4iDMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "f8fedd21-3179-4a62-9eca-e7a556419f89"
      },
      "source": [
        "updated_params, updated_gradients, costs = optimization(\n",
        "\twts_bias_parameters = initialized_params,\n",
        "\tX = X_train, Y = y_train, num_iterations = 1000,\n",
        "\tlearning_rate = 0.01, print_cost = True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss/Cost after 0 iterations = 0.6944\n",
            "\n",
            "\n",
            "Loss/Cost after 100 iterations = 0.6013\n",
            "\n",
            "\n",
            "Loss/Cost after 200 iterations = 0.3542\n",
            "\n",
            "\n",
            "Loss/Cost after 300 iterations = 0.2157\n",
            "\n",
            "\n",
            "Loss/Cost after 400 iterations = 0.1474\n",
            "\n",
            "\n",
            "Loss/Cost after 500 iterations = 0.1105\n",
            "\n",
            "\n",
            "Loss/Cost after 600 iterations = 0.0880\n",
            "\n",
            "\n",
            "Loss/Cost after 700 iterations = 0.0731\n",
            "\n",
            "\n",
            "Loss/Cost after 800 iterations = 0.0626\n",
            "\n",
            "\n",
            "Loss/Cost after 900 iterations = 0.0548\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0i3lnB0iIKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make predictions on test set-\n",
        "y_predictions = predict(updated_params, X_test)\n",
        "y_predictions = np.squeeze(y_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqO2K46AiLLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "361224fa-1f30-401a-8d38-9b973c30da13"
      },
      "source": [
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "precision = precision_score(y_test, y_predictions)\n",
        "recall = recall_score(y_test, y_predictions)\n",
        "\n",
        "print(\"\\nTrained Neural Network model metrics are: \")\n",
        "print(\"accuracy = {0:.4f}, precision = {1:.4f} & recall = {2:.4f}\\n\".format(accuracy, precision, recall))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Trained Neural Network model metrics are: \n",
            "accuracy = 0.9882, precision = 1.0000 & recall = 0.9763\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JJrfzbHiaPC",
        "colab_type": "text"
      },
      "source": [
        "**Now, new parameters are which in which all weights & biases are zeros**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOyu6Z_hj1b_",
        "colab_type": "text"
      },
      "source": [
        "**Observation:**\n",
        "If *both* weights and biases are set to zero, then no learning happens!\n",
        "\n",
        "However, if weights are zeros, but biases are non-zeros, then learning happens, however, the Neural Network will take more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJS3fsW4ihEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initialized_params_all_zeros = initialize_parameters_zeros(\n",
        "\tX_train.shape[1], n_hidden_neurons, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROKpBttOjgx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9bd4ebbf-2319-444e-de39-bdcdde0f4f47"
      },
      "source": [
        "initialized_params_all_zeros['b1']"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.01548721,  0.00743888,  0.0115279 , -0.02497696,  0.00829825,\n",
              "        -0.00037653,  0.00081903,  0.00749686,  0.00088415,  0.01137796]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YStoh9Y_jj2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fed419b7-f4b1-468b-fb41-cbe89a13ea44"
      },
      "source": [
        "initialized_params_all_zeros['b2']"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.00050823]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKnKJ_aciiwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "3a86ca7f-27dc-4514-cf00-35076ba5a54a"
      },
      "source": [
        "updated_params_zeros, updated_gradients_zeros, costs_zeros = optimization(\n",
        "\twts_bias_parameters = initialized_params_all_zeros,\n",
        "\tX = X_train, Y = y_train, num_iterations = 1500,\n",
        "\tlearning_rate = 0.01, print_cost = True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss/Cost after 0 iterations = 0.6931\n",
            "\n",
            "\n",
            "Loss/Cost after 100 iterations = 0.6931\n",
            "\n",
            "\n",
            "Loss/Cost after 200 iterations = 0.6931\n",
            "\n",
            "\n",
            "Loss/Cost after 300 iterations = 0.6921\n",
            "\n",
            "\n",
            "Loss/Cost after 400 iterations = 0.5950\n",
            "\n",
            "\n",
            "Loss/Cost after 500 iterations = 0.3223\n",
            "\n",
            "\n",
            "Loss/Cost after 600 iterations = 0.1698\n",
            "\n",
            "\n",
            "Loss/Cost after 700 iterations = 0.1027\n",
            "\n",
            "\n",
            "Loss/Cost after 800 iterations = 0.0705\n",
            "\n",
            "\n",
            "Loss/Cost after 900 iterations = 0.0530\n",
            "\n",
            "\n",
            "Loss/Cost after 1000 iterations = 0.0425\n",
            "\n",
            "\n",
            "Loss/Cost after 1100 iterations = 0.0354\n",
            "\n",
            "\n",
            "Loss/Cost after 1200 iterations = 0.0303\n",
            "\n",
            "\n",
            "Loss/Cost after 1300 iterations = 0.0265\n",
            "\n",
            "\n",
            "Loss/Cost after 1400 iterations = 0.0236\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByQnbc8zikRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51rtsHYYiMoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}